{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVwAXyJZdEaY",
        "outputId": "2dc61810-b7f7-4c7f-e9a0-600395ddff25"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.8.2\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
        "  if [ $GPU -eq 1 ]; then\n",
        "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia\n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Np30veRQorhh",
        "outputId": "f3bf948f-c119-4aa2-c250-61c28ea11a76"
      },
      "outputs": [],
      "source": [
        "[deps]\n",
        "Colors = \"5ae59095-9a9b-59fe-a467-6f913c188581\"\n",
        "Convex = \"f65535da-76fb-5f13-bab9-19810c17039a\"\n",
        "ECOS = \"e2685f51-7e38-5353-a97d-a921fd2c8199\"\n",
        "FiniteDiff = \"6a86dc24-6348-571c-b903-95158fe2bd41\"\n",
        "ForwardDiff = \"f6369f11-7733-5829-9624-2563aa707210\"\n",
        "Interpolations = \"a98d9a8b-a2ab-59e6-89dd-64a1c18fca59\"\n",
        "Ipopt = \"b6b21f68-93f8-5de0-b562-5493be1d77c9\"\n",
        "JLD2 = \"033835bb-8acc-5ee8-8aae-3f567f8a3819\"\n",
        "MathOptInterface = \"b8f27783-ece8-5eb3-8dc8-9495eed66fee\"\n",
        "MeshCat = \"283c5d60-a78f-5afe-a0af-af636b173e11\"\n",
        "Plots = \"91a5bcdd-55d7-5caf-9e0b-520d859cae80\"\n",
        "StaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpMJHm2YcFmH",
        "outputId": "2abd676a-3883-4b25-c8d3-1c1f6e6ec292"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/16745/final_proj/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/16745/final_proj/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/16745/final_proj/Project.toml`\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/16745/final_proj/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/16745/final_proj/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/16745/final_proj/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/16745/final_proj/Manifest.toml`\n"
          ]
        }
      ],
      "source": [
        "import Pkg\n",
        "Pkg.resolve()\n",
        "\n",
        "Pkg.activate(@__DIR__)\n",
        "Pkg.instantiate()\n",
        "Pkg.add(\"Interpolations\")\n",
        "Pkg.add(\"Dierckx\")\n",
        "import Interpolations\n",
        "import Dierckx\n",
        "import FiniteDiff\n",
        "import ForwardDiff as FD\n",
        "import Convex as cvx\n",
        "import ECOS\n",
        "using Dierckx\n",
        "using Interpolations\n",
        "using LinearAlgebra\n",
        "using Plots\n",
        "using Random\n",
        "using JLD2\n",
        "using Test\n",
        "using MeshCat\n",
        "const mc = MeshCat\n",
        "using StaticArrays\n",
        "using Printf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ5m283ge4S8",
        "outputId": "9fb97db1-5e65-4a56-9c7c-0683917ba807"
      },
      "outputs": [],
      "source": [
        "versioninfo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8EUikLdBod8",
        "outputId": "dec05caf-d9c5-4076-a720-efbd8363a60f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "fmincon (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from HW 3\n",
        "\n",
        "import MathOptInterface as MOI\n",
        "import Ipopt\n",
        "import FiniteDiff\n",
        "import ForwardDiff\n",
        "using LinearAlgebra\n",
        "\n",
        "struct ProblemMOI <: MOI.AbstractNLPEvaluator\n",
        "    n_nlp::Int\n",
        "    m_nlp::Int\n",
        "    obj_grad::Bool\n",
        "    con_jac::Bool\n",
        "    sparsity_jac\n",
        "    sparsity_hess\n",
        "    hessian_lagrangian::Bool\n",
        "    params::NamedTuple\n",
        "    cost # ::Function\n",
        "    con # ::Function\n",
        "    diff_type # :Symbol\n",
        "end\n",
        "\n",
        "function ProblemMOI(n_nlp,m_nlp,params,cost,con,diff_type;\n",
        "        obj_grad=true,\n",
        "        con_jac=true,\n",
        "        sparsity_jac=sparsity_jacobian(n_nlp,m_nlp),\n",
        "        sparsity_hess=sparsity_hessian(n_nlp,m_nlp),\n",
        "        hessian_lagrangian=false)\n",
        "\n",
        "    ProblemMOI(n_nlp,m_nlp,\n",
        "        obj_grad,\n",
        "        con_jac,\n",
        "        sparsity_jac,\n",
        "        sparsity_hess,\n",
        "        hessian_lagrangian,\n",
        "        params,\n",
        "        cost,\n",
        "        con,\n",
        "        diff_type)\n",
        "end\n",
        "\n",
        "function row_col!(row,col,r,c)\n",
        "    for cc in c\n",
        "        for rr in r\n",
        "            push!(row,convert(Int,rr))\n",
        "            push!(col,convert(Int,cc))\n",
        "        end\n",
        "    end\n",
        "    return row, col\n",
        "end\n",
        "\n",
        "function sparsity_jacobian(n,m)\n",
        "\n",
        "    row = []\n",
        "    col = []\n",
        "\n",
        "    r = 1:m\n",
        "    c = 1:n\n",
        "\n",
        "    row_col!(row,col,r,c)\n",
        "\n",
        "    return collect(zip(row,col))\n",
        "end\n",
        "\n",
        "function sparsity_hessian(n,m)\n",
        "\n",
        "    row = []\n",
        "    col = []\n",
        "\n",
        "    r = 1:m\n",
        "    c = 1:n\n",
        "\n",
        "    row_col!(row,col,r,c)\n",
        "\n",
        "    return collect(zip(row,col))\n",
        "end\n",
        "\n",
        "function MOI.eval_objective(prob::MOI.AbstractNLPEvaluator, x)\n",
        "    prob.cost(prob.params, x)\n",
        "end\n",
        "\n",
        "function MOI.eval_objective_gradient(prob::MOI.AbstractNLPEvaluator, grad_f, x)\n",
        "    _cost(_x) = prob.cost(prob.params, _x)\n",
        "    if prob.diff_type == :auto\n",
        "        ForwardDiff.gradient!(grad_f,_cost,x)\n",
        "    else\n",
        "        FiniteDiff.finite_difference_gradient!(grad_f, _cost, x)\n",
        "    end\n",
        "    return nothing\n",
        "end\n",
        "\n",
        "function MOI.eval_constraint(prob::MOI.AbstractNLPEvaluator,c,x)\n",
        "    c .= prob.con(prob.params, x)\n",
        "    return nothing\n",
        "end\n",
        "\n",
        "function MOI.eval_constraint_jacobian(prob::MOI.AbstractNLPEvaluator, jac, x)\n",
        "    _con(_x) = prob.con(prob.params, _x)\n",
        "    if prob.diff_type == :auto\n",
        "        reshape(jac,prob.m_nlp,prob.n_nlp) .= ForwardDiff.jacobian(_con, x)\n",
        "    else\n",
        "        reshape(jac,prob.m_nlp,prob.n_nlp) .= FiniteDiff.finite_difference_jacobian(_con, x)\n",
        "    end\n",
        "    return nothing\n",
        "end\n",
        "\n",
        "function MOI.features_available(prob::MOI.AbstractNLPEvaluator)\n",
        "    return [:Grad, :Jac]\n",
        "end\n",
        "\n",
        "MOI.initialize(prob::MOI.AbstractNLPEvaluator, features) = nothing\n",
        "MOI.jacobian_structure(prob::MOI.AbstractNLPEvaluator) = prob.sparsity_jac\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "x = fmincon(cost,equality_constraint,inequality_constraint,x_l,x_u,c_l,c_u,x0,params,diff_type)\n",
        "\n",
        "This function uses IPOPT to minimize an objective function\n",
        "\n",
        "`cost(params, x)`\n",
        "\n",
        "With the following three constraints:\n",
        "\n",
        "`equality_constraint(params, x) = 0`\n",
        "`c_l <= inequality_constraint(params, x) <= c_u`\n",
        "`x_l <= x <= x_u`\n",
        "\n",
        "Problem specific parameters should be loaded into params::NamedTuple (things like\n",
        "cost weights, dynamics parameters, etc.).\n",
        "\n",
        "args:\n",
        "    cost::Function                    - objective function to be minimzed (returns scalar)\n",
        "    equality_constraint::Function     - c_eq(params, x) == 0\n",
        "    inequality_constraint::Function   - c_l <= c_ineq(params, x) c_u\n",
        "    x_l::Vector                       - x_l <= x <= x_u\n",
        "    x_u::Vector                       - x_l <= x <= x_u\n",
        "    c_l::Vector                       - c_l <= c_ineq(params, x) <= x_u\n",
        "    c_u::Vector                       - c_l <= c_ineq(params, x) <= x_u\n",
        "    x0::Vector                        - initial guess\n",
        "    params::NamedTuple                - problem parameters for use in costs/constraints\n",
        "    diff_type::Symbol                 - :auto for ForwardDiff, :finite for FiniteDiff\n",
        "\n",
        "optional args:\n",
        "    tol                               - optimality tolerance\n",
        "    c_tol                             - constraint violation tolerance\n",
        "    max_iters                         - max iterations\n",
        "    verbose                           - true for IPOPT output, false for nothing\n",
        "\n",
        "You should try and use :auto for your `diff_type` first, and only use :finite if you\n",
        "absolutely cannot get ForwardDiff to work.\n",
        "\n",
        "This function will run a few basic checks before sending the problem off to IPOPT to\n",
        "solve. The outputs of these checks will be reported as the following:\n",
        "\n",
        "---------checking dimensions of everything----------\n",
        "---------all dimensions good------------------------\n",
        "---------diff type set to :auto (ForwardDiff.jl)----\n",
        "---------testing objective gradient-----------------\n",
        "---------testing constraint Jacobian----------------\n",
        "---------successfully compiled both derivatives-----\n",
        "---------IPOPT beginning solve----------------------\n",
        "\n",
        "If you're getting stuck during the testing of one of the derivatives, try switching\n",
        "to FiniteDiff.jl by setting diff_type = :finite.\n",
        "\"\"\"\n",
        "\n",
        "function fmincon(cost::Function,\n",
        "                 equality_constraint::Function,\n",
        "                 inequality_constraint::Function,\n",
        "                 x_l::Vector,\n",
        "                 x_u::Vector,\n",
        "                 c_l::Vector,\n",
        "                 c_u::Vector,\n",
        "                 x0::Vector,\n",
        "                 params::NamedTuple,\n",
        "                 diff_type::Symbol;\n",
        "                 tol = 1e-4,\n",
        "                 c_tol = 1e-4,\n",
        "                 max_iters = 1_000,\n",
        "                 verbose = true)::Vector\n",
        "\n",
        "    n_primals = length(x0)\n",
        "    n_eq = length(equality_constraint(params, x0))\n",
        "    n_ineq = length(inequality_constraint(params, x0))\n",
        "\n",
        "    verbose && println(\"---------checking dimensions of everything----------\")\n",
        "    @assert length(x0) == length(x_l) == length(x_u)\n",
        "    @assert length(c_l) == length(c_u) == n_ineq\n",
        "    @assert all(x_u .>= x_l)\n",
        "    if n_ineq > 0\n",
        "        @assert all(c_u .>= c_l)\n",
        "    end\n",
        "    verbose && println(\"---------all dimensions good------------------------\")\n",
        "\n",
        "\n",
        "    function con(params, x)\n",
        "        [\n",
        "            equality_constraint(params, x);\n",
        "            inequality_constraint(params, x)\n",
        "        ]\n",
        "    end\n",
        "\n",
        "    if diff_type == :auto\n",
        "        verbose && println(\"---------diff type set to :auto (ForwardDiff.jl)----\")\n",
        "    else\n",
        "        verbose && println(\"---------diff type set to :finite (FiniteDiff.jl)---\")\n",
        "    end\n",
        "    verbose && println(\"---------testing objective gradient-----------------\")\n",
        "    if diff_type == :auto\n",
        "        ForwardDiff.gradient(_x -> cost(params, _x), x0)\n",
        "    else\n",
        "        FiniteDiff.finite_difference_gradient(_x -> cost(params, _x), x0)\n",
        "    end\n",
        "    verbose && println(\"---------testing constraint Jacobian----------------\")\n",
        "    if diff_type == :auto\n",
        "        ForwardDiff.jacobian(_x -> con(params, _x), x0)\n",
        "    else\n",
        "        FiniteDiff.finite_difference_jacobian(_x -> con(params, _x), x0)\n",
        "    end\n",
        "    verbose && println(\"---------successfully compiled both derivatives-----\")\n",
        "\n",
        "    prob = ProblemMOI(n_primals, n_eq + n_ineq, params, cost, con, diff_type)\n",
        "\n",
        "    # add zeros(n_eq) for equality constraint\n",
        "    nlp_bounds = MOI.NLPBoundsPair.([zeros(n_eq); c_l],[zeros(n_eq); c_u])\n",
        "    block_data = MOI.NLPBlockData(nlp_bounds, prob, true)\n",
        "\n",
        "    solver = Ipopt.Optimizer()\n",
        "    solver.options[\"max_iter\"] = max_iters\n",
        "    solver.options[\"tol\"] = tol\n",
        "    solver.options[\"constr_viol_tol\"] = c_tol\n",
        "\n",
        "    if verbose\n",
        "        solver.options[\"print_level\"] = 5\n",
        "    else\n",
        "        solver.options[\"print_level\"] = 0\n",
        "    end\n",
        "\n",
        "    x = MOI.add_variables(solver,prob.n_nlp)\n",
        "\n",
        "    for i = 1:prob.n_nlp\n",
        "        MOI.add_constraint(solver, x[i], MOI.LessThan(x_u[i]))\n",
        "        MOI.add_constraint(solver, x[i], MOI.GreaterThan(x_l[i]))\n",
        "        MOI.set(solver, MOI.VariablePrimalStart(), x[i], x0[i])\n",
        "    end\n",
        "\n",
        "    # Solve the problem\n",
        "    verbose && println(\"---------IPOPT beginning solve----------------------\")\n",
        "\n",
        "    MOI.set(solver, MOI.NLPBlock(), block_data)\n",
        "    MOI.set(solver, MOI.ObjectiveSense(), MOI.MIN_SENSE)\n",
        "    MOI.optimize!(solver)\n",
        "\n",
        "    # Get the solution\n",
        "    res = MOI.get(solver, MOI.VariablePrimal(), x)\n",
        "\n",
        "    return res\n",
        "\n",
        "end\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziXow4fncFmJ",
        "notebookRunGroups": {
          "groupValue": ""
        },
        "outputId": "fb8030be-4370-459c-84da-73eb332f1796"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "rk4 (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "function dynamics(model::NamedTuple, x::Vector, u::Vector)::Vector\n",
        "    # kinematic bicycle model continuous time dynamics\n",
        "    # lr = distance from rear axle to COM\n",
        "    # L = wheelbase\n",
        "\n",
        "    lr, L = model.lr, model.L\n",
        "    # reftraj, reftraj_deriv, reftraj_2nd_deriv =\n",
        "    #     model.reftraj,\n",
        "    #     model.reftraj_deriv,\n",
        "    #     model.reftraj_2nd_deriv\n",
        "\n",
        "    # reftraj(s) = x, y\n",
        "    # a = a(s), which is experimentally determined\n",
        "\n",
        "    # along track, cross track, heading w.r.t track, steering angle, speed\n",
        "    s, r, e_psi, δ, v = x\n",
        "\n",
        "    # rate of steer\n",
        "    δdot, a = u\n",
        "    βdot = δdot / (1 + (lr * δ / L)^2)\n",
        "\n",
        "    sin_e_psi, cos_e_psi = sincos(e_psi)\n",
        "\n",
        "    # function T(_s)\n",
        "    #     reftrajdot = derivative(reftraj, _s)\n",
        "    #     reftrajdot / norm(reftrajdot)\n",
        "    # end\n",
        "\n",
        "    Tdot = reftraj_2nd_deriv(get_idx_from_dist(s))[1] / norm(reftraj_deriv(get_idx_from_dist(s))[1])\n",
        "\n",
        "    # curvature of reftraj\n",
        "    κ = norm(Tdot)\n",
        "\n",
        "    sdot = v * cos_e_psi / (1 - r * κ)\n",
        "    rdot = v * sin_e_psi\n",
        "    e_psi_dot = βdot + sdot * κ\n",
        "\n",
        "    xdot = [\n",
        "        sdot,\n",
        "        rdot,\n",
        "        e_psi_dot,\n",
        "        δdot,\n",
        "        a\n",
        "    ]\n",
        "    print(xdot)\n",
        "    return xdot\n",
        "end\n",
        "\n",
        "function rk4(model::NamedTuple, ode::Function, x::Vector, u::Vector, dt::Real)::Vector\n",
        "    k1 = dt * ode(model, x,        u)\n",
        "    k2 = dt * ode(model, x + k1/2, u)\n",
        "    k3 = dt * ode(model, x + k2/2, u)\n",
        "    k4 = dt * ode(model, x + k3,   u)\n",
        "    return x + (1/6)*(k1 + 2*k2 + 2*k3 + k4)\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "poGi9R2QcFmK",
        "outputId": "86805018-6350-4407-baa4-2a2d4439d4b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "get_dist_from_pos (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "function generate_reftraj(_X)::ParametricSpline\n",
        "    # https://juliahub.com/ui/Packages/General/Dierckx\n",
        "    xs = 1.0:size(_X, 1)\n",
        "    itp = ParametricSpline(xs, _X'; k=3, bc=\"extrapolate\")\n",
        "    return itp\n",
        "end\n",
        "\n",
        "X_mat = [0 0;0 1.0;0 2;0 4;0 9;0 16;0 25]\n",
        "X = [X_mat[i,:] for i in 1:size(X_mat,1)]\n",
        "reftraj_spl = generate_reftraj(X_mat)\n",
        "\n",
        "function matrix_to_vec(mat)\n",
        "    vec = [mat[i,:] for i in 1:size(mat,1)]\n",
        "    return vec\n",
        "end\n",
        "\n",
        "function reftraj(i)\n",
        "    mat = transpose(reftraj_spl(i))\n",
        "    return matrix_to_vec(mat)\n",
        "end\n",
        "\n",
        "function reftraj_deriv(i)\n",
        "    mat = transpose(derivative(reftraj_spl, i))\n",
        "    return matrix_to_vec(mat)\n",
        "end\n",
        "\n",
        "function reftraj_2nd_deriv(i)\n",
        "    mat = transpose(derivative(reftraj_spl, i; nu=2))\n",
        "    return matrix_to_vec(mat)\n",
        "end\n",
        "\n",
        "function generate_distances()\n",
        "    dt = 0.0001\n",
        "    ts = 0.0:dt:size(X, 1)\n",
        "    drdts = copy(reftraj_deriv(ts))\n",
        "    print(\"drdts shape\", size(drdts))\n",
        "    drdts = [drdts[i, :] for i in 1:size(drdts, 1)]\n",
        "    speeds = norm.(eachrow(drdts))\n",
        "\n",
        "    distances = zeros(size(speeds, 1))\n",
        "    distances[1] = speeds[1] * dt\n",
        "    for i = 2:size(drdts, 1)\n",
        "        distances[i] = speeds[i] * dt + distances[i - 1]\n",
        "    end\n",
        "\n",
        "    return distances, ts\n",
        "end\n",
        "distances, ts = generate_distances()\n",
        "print(\"size of dist\", size(distances), \"size of ts\", size(ts))\n",
        "\n",
        "# 1-indexed, so add 1 since ts are 0-indexed\n",
        "get_idx_from_dist(_d) =\n",
        "    1 + linear_interpolation(\n",
        "    distances,\n",
        "    ts,\n",
        "    extrapolation_bc=Interpolations.Line())(_d)\n",
        "\n",
        "get_dist_from_idx(_i) =\n",
        "    linear_interpolation(\n",
        "    ts,\n",
        "    distances,\n",
        "    extrapolation_bc=Interpolations.Line())(_i - 1)\n",
        "\n",
        "get_pos_from_dist(_d) =\n",
        "    reftraj(get_idx_from_dist(_d))\n",
        "\n",
        "function get_dist_from_pos(x)\n",
        "    # compute distance to each knot pt\n",
        "    dists_to_path = norm.(X .- Ref(x))\n",
        "    knot_idx = argmin(dists_to_path)\n",
        "    # subsample the distance function in the interval [knot idx - 1, knot idx + 1]\n",
        "    # for each subsample along the path, compute distance from x  to subsample\n",
        "    path_subsample = reftraj(knot_idx-1:0.01:knot_idx+1)\n",
        "    dists_to_subsample = norm.(path_subsample .- Ref(x))\n",
        "    knot_idx_subsample = argmin(dists_to_subsample) * 0.01 + (knot_idx - 1)\n",
        "    @show \"subsample range\", knot_idx-1:0.01:knot_idx+1\n",
        "    @show \"spline position along subsample range\", path_subsample\n",
        "    @show \"subsampled dists\" dists_to_subsample\n",
        "    @show \"closest idx\", knot_idx\n",
        "    @show \"closest subsample idx\", knot_idx_subsample\n",
        "    return get_dist_from_idx(knot_idx_subsample)\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adapted from HW4\n",
        "function create_idx(nx,nu,N)\n",
        "    # create idx for indexing convenience\n",
        "    # x_i = Z[idx.x[i]]\n",
        "    # u_i = Z[idx.u[i]]\n",
        "    # and stacked dynamics constraints of size nx are\n",
        "    # c[idx.c[i]] = <dynamics constraint at time step i>\n",
        "\n",
        "    # our Z vector is [x0, u0, x1, u1, …, xN]\n",
        "    nz = (N-1) * nu + N * nx # length of Z\n",
        "    x = [(i - 1) * (nx + nu) .+ (1 : nx) for i = 1:N]\n",
        "    u = [(i - 1) * (nx + nu) .+ ((nx + 1):(nx + nu)) for i = 1:(N - 1)]\n",
        "\n",
        "    # constraint indexing for the (N-1) dynamics constraints when stacked up\n",
        "    c = [(i - 1) * (nx) .+ (1 : nx) for i = 1:(N - 1)]\n",
        "    nc = (N - 1) * nx # (N-1)*nx\n",
        "\n",
        "    return (nx=nx,nu=nu,N=N,nz=nz,nc=nc,x= x,u = u,c = c)\n",
        "end\n",
        "\n",
        "function dynamics_constraints(params, Z)\n",
        "    idx = params.idx\n",
        "    N = params.N\n",
        "    c = zeros(eltype(Z), idx.nc)\n",
        "    model = params.model\n",
        "    dt = params.dt\n",
        "\n",
        "    for i = 1:N-1\n",
        "        xi = Z[idx.x[i]]\n",
        "        xi1 = Z[idx.x[i + 1]]\n",
        "        ui = Z[idx.u[i]]\n",
        "        c[idx.c[i]] = rk4(model, dynamics, xi, ui, dt) - xi1\n",
        "    end\n",
        "\n",
        "    return c\n",
        "end\n",
        "\n",
        "function equality_constraints(params, Z)\n",
        "    idx, xi = params.idx, params.xi\n",
        "    # TODO: stack up all of our equality constraints\n",
        "    # print(\"xi shape, x shape \", size(xi), size(Z[idx.x[1]]))\n",
        "    init_constraint = xi - Z[idx.x[1]]\n",
        "    return init_constraint\n",
        "end\n",
        "\n",
        "function inequality_constraints(params, Z)\n",
        "    return [0]\n",
        "end\n",
        "\n",
        "function cost(params::NamedTuple, Z::Vector)::Real\n",
        "    # cost function\n",
        "    idx, N, s_goal = params.idx, params.N, params.s_goal\n",
        "    Q, R = params.Q, params.R\n",
        "\n",
        "    J = 0\n",
        "    for i = 1:N-1\n",
        "        Xe = Z[idx.x[i]] - [s_goal, 0, 0, 0, 0]\n",
        "        U = Z[idx.u[i]]\n",
        "        J += Xe' * Q * Xe + U' * R * U\n",
        "    end\n",
        "\n",
        "    Xfe = Z[idx.x[N]] - [s_goal, 0, 0, 0, 0]\n",
        "    J += Xfe' * Q * Xfe\n",
        "\n",
        "    return 0.5 * J\n",
        "end\n",
        "\n",
        "function optimize(Xpath)\n",
        "    dt = 0.001 #s\n",
        "    t_horizon = 2 #s\n",
        "    N = Int(t_horizon / dt)\n",
        "\n",
        "    model = (L = 1.2,\n",
        "        lr = 0.4,\n",
        "        reftraj_2nd_deriv=reftraj_2nd_deriv,\n",
        "        reftraj_deriv=reftraj_deriv,\n",
        "        reftraj=reftraj)\n",
        "\n",
        "    track = (width = 3.0)\n",
        "    idx = create_idx(5, 2, N)\n",
        "\n",
        "    Q = diagm([100; 0; 0; 0; 0])\n",
        "    R = diagm([0.1; 00.1])\n",
        "    # print(\"len x, reftraj at len x \", length(Xpath), reftraj(length(Xpath)))\n",
        "    s_goal = get_dist_from_idx(length(Xpath))\n",
        "    # print(\"sgoal\", s_goal)\n",
        "\n",
        "    @show \"Xpath\", Xpath[1]\n",
        "    si = get_dist_from_pos(Xpath[1])\n",
        "\n",
        "\n",
        "    print(get_pos_from_dist(si), Xpath[1])\n",
        "    # get pos from dist returns N x 2\n",
        "    # N is 1 here so still need to index into the return value\n",
        "    pos_to_ref = get_pos_from_dist(si)[1] - Xpath[1]\n",
        "\n",
        "    # check sign via cross product with tangent\n",
        "    tangent = reftraj_deriv(1)[1]\n",
        "    @show size(tangent)\n",
        "    cross_product = (tangent[1] * pos_to_ref[2]) - (tangent[2] * pos_to_ref[1])\n",
        "    r_norm = norm(tangent[1] - pos_to_ref[2])\n",
        "    # TODO: figure out sign convention\n",
        "    ri = sign(cross_product) * r_norm\n",
        "    xi = [si, ri, 0, 0, 0]\n",
        "    @show (\"xi: \", xi)\n",
        "\n",
        "    params = (\n",
        "        model=model,\n",
        "        track=track,\n",
        "        idx=idx,\n",
        "        N=N,\n",
        "        Q=Q,\n",
        "        R=R,\n",
        "        dt=dt,\n",
        "        xi=xi,\n",
        "        s_goal=s_goal)\n",
        "    z0 = zeros(idx.nz)\n",
        "    # TODO: if this isn't good enough, do constant velocity and get states from reftraj\n",
        "    # and controls from curvature\n",
        "\n",
        "    # adding a little noise to the initial guess is a good idea\n",
        "    # z0 = z0 + (1e-6)*randn(idx.nz)\n",
        "\n",
        "    # TODO: primal bounds\n",
        "\n",
        "    x_l = -Inf*ones(idx.nz) # update this\n",
        "    x_u =  Inf*ones(idx.nz) # update this\n",
        "\n",
        "    for i = 1:N-1\n",
        "        # cross track\n",
        "        x_l[idx.x[i+1][2]] = -3.0\n",
        "        x_u[idx.x[i+1][2]] = 3.0\n",
        "\n",
        "        # accelerate/brake limits\n",
        "        x_l[idx.u[i][1]] = -3.0\n",
        "        x_u[idx.u[i][1]] = 3.0\n",
        "\n",
        "        # steer rate limits\n",
        "        x_l[idx.u[i][2]] = -1.0\n",
        "        x_u[idx.u[i][2]] = 1.0\n",
        "    end\n",
        "\n",
        "    # inequality constraint bounds\n",
        "    c_l =  -Inf*ones(1)\n",
        "    c_u =   Inf*ones(1)\n",
        "    diff_type = :auto\n",
        "\n",
        "    Z = fmincon(cost,equality_constraints,inequality_constraints,\n",
        "                x_l,x_u,c_l,c_u,z0,params, diff_type;\n",
        "                tol = 1e-6, c_tol = 1e-6, max_iters = 10_000, verbose = true)\n",
        "\n",
        "    xi = [17.5, 0, 0, 0, 0]\n",
        "    ui = [0.0, 0]\n",
        "    print(rk4(model, dynamics, xi, ui, dt))\n",
        "\n",
        "    for i = 1:N-1\n",
        "        print(i, \"state:\", Z[idx.x[i]])\n",
        "        print(\"ctrl:\", Z[idx.u[i]])\n",
        "        print(\"\\n\")\n",
        "    end\n",
        "    print(\"final state\", Z[idx.x[N]])\n",
        "    print(\"\\n\")\n",
        "end\n",
        "\n",
        "optimize(X)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Julia 1.6.7",
      "language": "julia",
      "name": "julia-1.6"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
